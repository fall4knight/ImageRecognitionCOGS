{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feed-Foward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import get_q1_data\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 8,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 training samples, 30 test samples\n",
      "classes: [b'Iris-versicolor' b'Iris-virginica']\n",
      "The first 10 training samples are (with bias):\n",
      "[[1.  5.6 3.  4.1 1.3]\n",
      " [1.  5.5 2.5 4.  1.3]\n",
      " [1.  5.5 2.6 4.4 1.2]\n",
      " [1.  6.1 3.  4.6 1.4]\n",
      " [1.  5.8 2.6 4.  1.2]\n",
      " [1.  5.  2.3 3.3 1. ]\n",
      " [1.  5.6 2.7 4.2 1.3]\n",
      " [1.  5.7 3.  4.2 1.2]\n",
      " [1.  5.7 2.9 4.2 1.3]\n",
      " [1.  6.2 2.9 4.3 1.3]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, le = get_q1_data()\n",
    "print(\"%d training samples, %d test samples\"%(X_train.shape[0], X_test.shape[0]))\n",
    "print(\"classes:\", le.classes_)\n",
    "print(\"The first 10 training samples are (with bias):\")\n",
    "print(X_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement sigmoid function\n",
    "\\begin{align}\n",
    "sigmoid(x) & = \\frac{1}{1+e^{-x}} \\\\\n",
    "\\end{align}\n",
    "<img src=\"Figures/logistic.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    ### TODO: Fill this function with your implementation of sigmoid function ####\n",
    "    return 1.0 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement cross entropy\n",
    "For binary classification for all samples with the output vector o and target label t $\\in \\{0, 1\\}$:\n",
    "\\begin{align}\n",
    "L(o, t) & = - \\sum_{i=1}^n(t^{(i)}log(o^{(i)}) + (1-t^{i})log(1-o^{i})) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy(o,t):\n",
    "    ### o is the output, t is the target.\n",
    "    ### TODO: Fill this function with your implementation of crossentropy function for all samples ####\n",
    "    return -1.0 * np.sum(t * np.log(o) + (1 - t) * np.log(1 - o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initialize weights\n",
    "For weight initialization, please refer to http://cs231n.github.io/neural-networks-2/#init.\n",
    "\n",
    "Here we are building a feed forward neural network with 2 hidden units as shown below. \n",
    "<img src=\"Figures/nn.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "J = 2 # number of hidden units\n",
    "### TODO: Fill the information for weight initialization ###\n",
    "w1 = np.random.randn(5, 2) / sqrt(5) # initialize weights with calibration between input and hidden layer.\n",
    "w2 = np.random.randn(3, 1) / sqrt(3) # initialize weights with calibration between hidden and output layer.\n",
    "n_iter = 10000 # can be modified\n",
    "alpha = 0.002 # can be modified\n",
    "train_err = []\n",
    "test_err = []\n",
    "dw1_ = []\n",
    "train_loss = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement gradient descent for n iterations.\n",
    "Implement the update dw1 and dw2 based on your derivations for \\begin{align}\n",
    "\\frac{\\delta L}{\\delta w_2}, \n",
    "\\frac{\\delta L}{\\delta w_1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w1, w2):\n",
    "    o1 = sigmoid(w1.T.dot(X.T))\n",
    "    temp = np.vstack((o1, np.ones((1, o1.shape[1]))))\n",
    "    o2 = sigmoid(w2.T.dot(temp))\n",
    "    return o2\n",
    "\n",
    "def err(yp, y):\n",
    "    yp = [1 if yp[i] >= 0.5 else 0 for i in range(len(yp))]\n",
    "    return 1 - sum([1 if yp[i] == y[i] else 0 for i in range(len(yp))]) * 1.0 / len(yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### TODO: Fill the blanks below for gradient descent ###\n",
    "for n in range(n_iter):\n",
    "    # forward computation\n",
    "    # compute the predictions\n",
    "    o1 = sigmoid(X_train.dot(w1))\n",
    "    temp = np.hstack((o1, np.ones((X_train.shape[0], 1))))\n",
    "    o2 = sigmoid(temp.dot(w2))\n",
    "    # backward computation to calculate dw1 and dw2\n",
    "    dw2 = temp.T.dot(y_train - o2)\n",
    "    dw1 = X_train.T.dot((y_train - o2).dot(w2[:2].T)*((o1*(1-o1))))\n",
    "    # weight updating\n",
    "    w1 = w1 + alpha*dw1\n",
    "    w2 = w2 + alpha*dw2\n",
    "    # training error\n",
    "    y_predict = o2\n",
    "    train_err.append(err(y_predict, y_train)) # calculate the error and append to train_err\n",
    "    # training loss\n",
    "    train_loss.append(crossentropy(y_predict, y_train)) # use your crossentropy to calculate the loss\n",
    "    # test error\n",
    "    y_predict = predict(X_test, w1, w2).T\n",
    "    test_err.append(err(y_predict, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Print training loss vs number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Print training error and test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.1333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(train_err[-1], test_err[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Char RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\heath\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocab: 14399\n"
     ]
    }
   ],
   "source": [
    "rawtext = open('tinyshakespeare.txt','r').read().split('\\n')\n",
    "rawtext = ' '.join(rawtext)\n",
    "rawtext = [word.strip(string.punctuation) for word in rawtext.split()]\n",
    "rawtext = ' '.join(rawtext)\n",
    "rawtext = rawtext.replace('-', ' ')\n",
    "rawtext = ' '.join(rawtext.split())\n",
    "all_words = rawtext.split()\n",
    "unique_words = sorted(list(set(all_words)))\n",
    "n_vocab = len(unique_words)\n",
    "print(\"Total Vocab:\", n_vocab)\n",
    "word_to_int = dict((w, i) for i, w in enumerate(unique_words))\n",
    "int_to_word = dict((i, w) for i, w in enumerate(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203851\n"
     ]
    }
   ],
   "source": [
    "raw_text = rawtext.split()\n",
    "n_words = len(raw_text)\n",
    "print(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns: 203751\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_words - seq_length):\n",
    "    seq_in  = raw_text[i: i+seq_length]\n",
    "    seq_out = raw_text[i+seq_length]\n",
    "    dataX.append([word_to_int[word] for word in seq_in])\n",
    "    dataY.append(word_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print('Total patterns:', n_patterns)\n",
    "X_train = np.reshape(dataX, (n_patterns, seq_length, 1))/float(n_vocab)\n",
    "Y_train = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((203751, 100, 1), (203751, 14399))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14399)             3700543   \n",
      "=================================================================\n",
      "Total params: 3,964,735\n",
      "Trainable params: 3,964,735\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Y_train.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint# defin \n",
    "filepath=\"word-weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\heath\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "model_info = model.fit(X_train[:20000], Y_train[:20000], nb_epoch=10, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history(model_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Object Detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
