{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feed-Foward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import get_q1_data\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 8,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 training samples, 30 test samples\n",
      "classes: [b'Iris-versicolor' b'Iris-virginica']\n",
      "The first 10 training samples are (with bias):\n",
      "[[1.  5.6 3.  4.1 1.3]\n",
      " [1.  5.5 2.5 4.  1.3]\n",
      " [1.  5.5 2.6 4.4 1.2]\n",
      " [1.  6.1 3.  4.6 1.4]\n",
      " [1.  5.8 2.6 4.  1.2]\n",
      " [1.  5.  2.3 3.3 1. ]\n",
      " [1.  5.6 2.7 4.2 1.3]\n",
      " [1.  5.7 3.  4.2 1.2]\n",
      " [1.  5.7 2.9 4.2 1.3]\n",
      " [1.  6.2 2.9 4.3 1.3]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, le = get_q1_data()\n",
    "print(\"%d training samples, %d test samples\"%(X_train.shape[0], X_test.shape[0]))\n",
    "print(\"classes:\", le.classes_)\n",
    "print(\"The first 10 training samples are (with bias):\")\n",
    "print(X_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement sigmoid function\n",
    "\\begin{align}\n",
    "sigmoid(x) & = \\frac{1}{1+e^{-x}} \\\\\n",
    "\\end{align}\n",
    "<img src=\"Figures/logistic.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    ### TODO: Fill this function with your implementation of sigmoid function ####\n",
    "    return 1.0 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement cross entropy\n",
    "For binary classification for all samples with the output vector o and target label t $\\in \\{0, 1\\}$:\n",
    "\\begin{align}\n",
    "L(o, t) & = - \\sum_{i=1}^n(t^{(i)}log(o^{(i)}) + (1-t^{i})log(1-o^{i})) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def crossentropy(o,t):\n",
    "    ### o is the output, t is the target.\n",
    "    ### TODO: Fill this function with your implementation of crossentropy function for all samples ####\n",
    "    return -1.0 * np.sum(t * np.log(o) + (1 - t) * np.log(1 - o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initialize weights\n",
    "For weight initialization, please refer to http://cs231n.github.io/neural-networks-2/#init.\n",
    "\n",
    "Here we are building a feed forward neural network with 2 hidden units as shown below. \n",
    "<img src=\"Figures/nn.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "J = 2 # number of hidden units\n",
    "### TODO: Fill the information for weight initialization ###\n",
    "w1 = np.random.randn(5, 2) / sqrt(5) # initialize weights with calibration between input and hidden layer.\n",
    "w2 = np.random.rand(3, 1) / sqrt(3) # initialize weights with calibration between hidden and output layer.\n",
    "n_iter = 10000 # can be modified\n",
    "alpha = 0.002 # can be modified\n",
    "train_err = []\n",
    "test_err = []\n",
    "dw1_ = []\n",
    "train_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 2), (3, 1))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.shape, w2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement gradient descent for n iterations.\n",
    "Implement the update dw1 and dw2 based on your derivations for \\begin{align}\n",
    "\\frac{\\delta L}{\\delta w_2}, \n",
    "\\frac{\\delta L}{\\delta w_1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (70,70) (70,5) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-da86105a95e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# backward computation to calculate dw1 and dw2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# compute the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mdw1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mo2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mo1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mo1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mdw2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mo2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mo1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0m_______________\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (70,70) (70,5) "
     ]
    }
   ],
   "source": [
    "### TODO: Fill the blanks below for gradient descent ###\n",
    "for n in range(n_iter):\n",
    "    # forward computation\n",
    "    # compute the predictions\n",
    "    o1 = sigmoid(w1.T.dot(X_train.T))\n",
    "    temp = np.vstack((np.ones((1, o1.shape[1])), o1)) \n",
    "    o2 = sigmoid(w2.T.dot(temp))\n",
    "    # backward computation to calculate dw1 and dw2\n",
    "    # compute the loss\n",
    "    dw1 = (o2 - y_train) * o1.T.dot(1 - o1) * X_train\n",
    "    dw2 = (o2 - y_train) * o1\n",
    "    _______________\n",
    "    _______________\n",
    "    _______________\n",
    "    # weight updating\n",
    "    w1 = w1 + alpha*dw1\n",
    "    w2 = w2 + alpha*dw2\n",
    "    # training error\n",
    "    y_predict = _________\n",
    "    train_err.append(____) # calculate the error and append to train_err\n",
    "    # training loss\n",
    "    train_loss.append(____) # use your crossentropy to calculate the loss\n",
    "    # test error\n",
    "    __________________\n",
    "    __________________\n",
    "    __________________\n",
    "    y_predict = __________\n",
    "    test_err.append(______)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Print training loss vs number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Print training error and test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_err, label=\"Training error\")\n",
    "plt.plot(test_err, label=\"Test error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Char RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\heath\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, Activation\n",
    "from keras import optimizers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (None, 50)                2600      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 46)                2346      \n",
      "=================================================================\n",
      "Total params: 4,946\n",
      "Trainable params: 4,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(100, input_shape = (100,1), return_sequences = False))\n",
    "model.add(Dense(46, activation=\"softmax\"))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115390 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "data = open('tinyshakespeare.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 65)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wxh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Object Detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
